{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\todof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:1504\u001b[0m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdlpack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dlpack, to_dlpack\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# Import experimental masked operations support. See\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# information.\u001b[39;00m\n\u001b[1;32m-> 1504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m masked\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;66;03m# Import removed ops with error message about removal\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linalg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m     matrix_rank,\n\u001b[0;32m   1509\u001b[0m     eig,\n\u001b[0;32m   1510\u001b[0m     solve,\n\u001b[0;32m   1511\u001b[0m     lstsq,\n\u001b[0;32m   1512\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\todof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\masked\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaskedtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_masked_tensor, MaskedTensor\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaskedtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m as_masked_tensor, masked_tensor\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     _canonical_dim,\n\u001b[0;32m      5\u001b[0m     _generate_docstring,\n\u001b[0;32m      6\u001b[0m     _reduction_identity,\n\u001b[0;32m      7\u001b[0m     _where,\n\u001b[0;32m      8\u001b[0m     _input_mask,\n\u001b[0;32m      9\u001b[0m     _output_mask,\n\u001b[0;32m     10\u001b[0m     _combine_input_and_mask,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28msum\u001b[39m,\n\u001b[0;32m     12\u001b[0m     prod,\n\u001b[0;32m     13\u001b[0m     cumsum,\n\u001b[0;32m     14\u001b[0m     cumprod,\n\u001b[0;32m     15\u001b[0m     amax,\n\u001b[0;32m     16\u001b[0m     amin,\n\u001b[0;32m     17\u001b[0m     argmax,\n\u001b[0;32m     18\u001b[0m     argmin,\n\u001b[0;32m     19\u001b[0m     mean,\n\u001b[0;32m     20\u001b[0m     median,\n\u001b[0;32m     21\u001b[0m     logsumexp,\n\u001b[0;32m     22\u001b[0m     logaddexp,\n\u001b[0;32m     23\u001b[0m     norm,\n\u001b[0;32m     24\u001b[0m     var,\n\u001b[0;32m     25\u001b[0m     std,\n\u001b[0;32m     26\u001b[0m     softmax,\n\u001b[0;32m     27\u001b[0m     log_softmax,\n\u001b[0;32m     28\u001b[0m     softmin,\n\u001b[0;32m     29\u001b[0m     normalize,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_masked_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_masked_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasked_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaskedTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\todof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\masked\\_ops.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmasked\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m as_masked_tensor, is_masked_tensor, MaskedTensor\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _docs\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corresponding_real_dtype\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sym_float\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\Users\\todof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_prims_common\\__init__.py:23\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cmp_to_key, reduce\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     Any,\n\u001b[0;32m     12\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     Union,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sym_float, sym_int, sym_max\n",
      "File \u001b[1;32mc:\\Users\\todof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\__init__.py:258\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# This module causes conflicts with other modules:\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# from .stats import *\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# Adds about .04-.05 seconds of import time\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# from combinatorics import *\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# This module is slow to import:\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m#from physics import units\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot, textplot, plot_backends, plot_implicit, plot_parametric\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minteractive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_session, init_printing, interactive_traversal\n\u001b[0;32m    260\u001b[0m evalf\u001b[38;5;241m.\u001b[39m_create_evalf_table()\n\u001b[0;32m    262\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    264\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoctest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    505\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\todof\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\interactive\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helper module for setting up interactive SymPy sessions. \"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprinting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_printing\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_session\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraversal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interactive_traversal\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_printing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_session\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteractive_traversal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "# Import necessary functions from the original code\n",
    "from LLMs_attack import (\n",
    "    load_model, eval, gen_prompt, format_example,\n",
    "    move_answers_to_position, format_subject,\n",
    "    full_search_eval, reduce_choices_and_answer\n",
    ")\n",
    "\n",
    "# Create a directory for data\n",
    "os.makedirs('data/MMLU/dev', exist_ok=True)\n",
    "os.makedirs('data/MMLU/test', exist_ok=True)\n",
    "\n",
    "# Download a subset of MMLU dataset\n",
    "!wget -q -O data/MMLU/dev/abstract_algebra_dev.csv https://raw.githubusercontent.com/hendrycks/test/master/data/dev/abstract_algebra_dev.csv\n",
    "!wget -q -O data/MMLU/test/abstract_algebra_test.csv https://raw.githubusercontent.com/hendrycks/test/master/data/test/abstract_algebra_test.csv\n",
    "!wget -q -O data/MMLU/dev/anatomy_dev.csv https://raw.githubusercontent.com/hendrycks/test/master/data/dev/anatomy_dev.csv\n",
    "!wget -q -O data/MMLU/test/anatomy_test.csv https://raw.githubusercontent.com/hendrycks/test/master/data/test/anatomy_test.csv\n",
    "!wget -q -O data/MMLU/dev/computer_security_dev.csv https://raw.githubusercontent.com/hendrycks/test/master/data/dev/computer_security_dev.csv\n",
    "!wget -q -O data/MMLU/test/computer_security_test.csv https://raw.githubusercontent.com/hendrycks/test/master/data/test/computer_security_test.csv\n",
    "!wget -q -O data/MMLU/dev/high_school_mathematics_dev.csv https://raw.githubusercontent.com/hendrycks/test/master/data/dev/high_school_mathematics_dev.csv\n",
    "!wget -q -O data/MMLU/test/high_school_mathematics_test.csv https://raw.githubusercontent.com/hendrycks/test/master/data/test/high_school_mathematics_test.csv\n",
    "\n",
    "# Define custom Args object to pass to functions\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.ntrain = 5  # Number of examples to use for few-shot\n",
    "        self.data_dir = \"data/MMLU\"\n",
    "        self.engine = [\"gemma\", \"llama\", \"qwen\"]  # Models to evaluate\n",
    "        self.n_reduced = None  # No reduction in choices by default\n",
    "        self.use_subset = True  # Use a subset of the test data for faster evaluation\n",
    "        self.permutation_attack = False  # Don't use permutation attack by default\n",
    "        self.position_permute = False  # Don't use position permutation by default\n",
    "        self.reduce_attack = False  # Don't use reduce attack by default\n",
    "        self.load_in_8bit = False  # Don't load in 8-bit by default\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# ## Experiment 1: Basic MMLU Performance Comparison\n",
    "# \n",
    "# We'll compare the performance of three small models on a subset of MMLU benchmark.\n",
    "\n",
    "# Set subjects to evaluate\n",
    "subjects = ['abstract_algebra', 'anatomy', 'computer_security', 'high_school_mathematics']\n",
    "print(f\"Evaluating models on subjects: {subjects}\")\n",
    "\n",
    "# Modify args to use subset\n",
    "args.use_subset = True  # Use only 100 examples from test set\n",
    "args.ntrain = 5  # Use 5-shot examples for few-shot learning\n",
    "\n",
    "# Define results dictionary\n",
    "basic_results = {}\n",
    "\n",
    "# Set the models to evaluate\n",
    "args.engine = [\"gemma\", \"llama\", \"qwen\"]  # Using Qwen as third model instead of Mistral\n",
    "\n",
    "for engine in args.engine:\n",
    "    print(f\"\\n=====================================\")\n",
    "    print(f\"Engine: {engine}\")\n",
    "    print(f\"=====================================\")\n",
    "    \n",
    "    all_cors = []\n",
    "    all_accs = []\n",
    "    subject_results = {}\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model(args, engine)\n",
    "    \n",
    "    # Move to GPU if not using 8-bit quantization\n",
    "    if not args.load_in_8bit and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    for subject in subjects:\n",
    "        print(f\"Evaluating {subject}...\")\n",
    "        \n",
    "        # Load development and test data\n",
    "        dev_df = pd.read_csv(os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None)[:args.ntrain]\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None)\n",
    "        \n",
    "        # Use only first 20 examples for faster evaluation\n",
    "        test_df = test_df[:20]\n",
    "        \n",
    "        # Evaluate model\n",
    "        cors, acc = eval(args, format_subject(subject), dev_df, test_df, model, tokenizer, n_reduced=args.n_reduced)\n",
    "        \n",
    "        # Store results\n",
    "        all_cors.append(cors)\n",
    "        all_accs.append(acc)\n",
    "        subject_results[subject] = acc\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    weighted_acc = np.mean(np.concatenate(all_cors))\n",
    "    print(f\"Average accuracy: {weighted_acc*100:.2f}%\")\n",
    "    \n",
    "    # Store results\n",
    "    basic_results[engine] = {\n",
    "        'subject_accs': subject_results,\n",
    "        'overall_acc': weighted_acc\n",
    "    }\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Plot basic results\n",
    "plt.figure(figsize=(12, 6))\n",
    "engines = list(basic_results.keys())\n",
    "subjects_list = list(basic_results[engines[0]]['subject_accs'].keys())\n",
    "\n",
    "# Create bar chart\n",
    "x = np.arange(len(subjects_list))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "for engine in engines:\n",
    "    offset = width * multiplier\n",
    "    accs = [basic_results[engine]['subject_accs'][subject] * 100 for subject in subjects_list]\n",
    "    plt.bar(x + offset, accs, width, label=engine)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Subjects')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('MMLU Performance by Model and Subject (5-shot)')\n",
    "plt.xticks(x + width, [subject.replace('_', ' ').title() for subject in subjects_list], rotation=45)\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show overall accuracies\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(engines, [basic_results[engine]['overall_acc'] * 100 for engine in engines])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Overall MMLU Accuracy by Model (5-shot)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0, 50)  # Assuming accuracies are within this range\n",
    "\n",
    "# ## Experiment 2: Position Bias Analysis\n",
    "# \n",
    "# Testing if the models have a bias towards choosing answers at specific positions (A, B, C, D)\n",
    "\n",
    "# Choose one model for position bias analysis to save time\n",
    "args.engine = [\"gemma\"]  # Using Gemma for position bias analysis\n",
    "args.position_permute = True  # Enable position permutation\n",
    "\n",
    "position_results = {}\n",
    "engines = args.engine\n",
    "\n",
    "for engine in engines:\n",
    "    print(f\"\\n=====================================\")\n",
    "    print(f\"Position Bias Analysis for Engine: {engine}\")\n",
    "    print(f\"=====================================\")\n",
    "    \n",
    "    all_accs = []\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model(args, engine)\n",
    "    \n",
    "    # Move to GPU if not using 8-bit quantization\n",
    "    if not args.load_in_8bit and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    for subject in subjects:\n",
    "        print(f\"Evaluating {subject}...\")\n",
    "        \n",
    "        # Load development and test data\n",
    "        dev_df = pd.read_csv(os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None)[:args.ntrain]\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None)\n",
    "        \n",
    "        # Use only first 10 examples for faster evaluation\n",
    "        test_df = test_df[:10]\n",
    "        \n",
    "        # Test with answers at different positions\n",
    "        position_accs = {}\n",
    "        for position in ['A', 'B', 'C', 'D']:\n",
    "            print(f\"Testing with answers at position {position}...\")\n",
    "            new_df = move_answers_to_position(test_df, position)\n",
    "            cors, acc = eval(args, format_subject(subject), dev_df, new_df, model, tokenizer, n_reduced=args.n_reduced, permute_pos=position)\n",
    "            position_accs[position] = acc\n",
    "        \n",
    "        all_accs.append(position_accs)\n",
    "        print(f\"Position accuracies for {subject}: {position_accs}\")\n",
    "    \n",
    "    # Store results\n",
    "    position_results[engine] = all_accs\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Reset position_permute flag\n",
    "args.position_permute = False\n",
    "\n",
    "# Plot position bias results\n",
    "plt.figure(figsize=(10, 6))\n",
    "positions = ['A', 'B', 'C', 'D']\n",
    "\n",
    "# Calculate average accuracy for each position across all subjects\n",
    "avg_position_accs = {}\n",
    "for engine in position_results:\n",
    "    avg_position_accs[engine] = {pos: 0 for pos in positions}\n",
    "    for subject_result in position_results[engine]:\n",
    "        for pos in positions:\n",
    "            avg_position_accs[engine][pos] += subject_result[pos]\n",
    "    # Divide by number of subjects\n",
    "    for pos in positions:\n",
    "        avg_position_accs[engine][pos] /= len(subjects)\n",
    "\n",
    "# Create bar chart for position bias\n",
    "for engine in avg_position_accs:\n",
    "    plt.bar(positions, [avg_position_accs[engine][pos] * 100 for pos in positions], label=engine)\n",
    "    \n",
    "plt.xlabel('Answer Position')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Position Bias Analysis: Accuracy by Answer Position')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0, 50)  # Assuming accuracies are within this range\n",
    "\n",
    "# ## Experiment 3: Shot Analysis\n",
    "# \n",
    "# Testing how performance changes with different numbers of few-shot examples\n",
    "\n",
    "# Choose one model for shot analysis\n",
    "args.engine = [\"llama\"]  # Using Llama for shot analysis\n",
    "shot_counts = [0, 1, 3, 5]  # Different numbers of shots to test\n",
    "\n",
    "shot_results = {}\n",
    "engines = args.engine\n",
    "\n",
    "for engine in engines:\n",
    "    print(f\"\\n=====================================\")\n",
    "    print(f\"Shot Analysis for Engine: {engine}\")\n",
    "    print(f\"=====================================\")\n",
    "    \n",
    "    engine_results = {}\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model(args, engine)\n",
    "    \n",
    "    # Move to GPU if not using 8-bit quantization\n",
    "    if not args.load_in_8bit and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    for subject in subjects[:2]:  # Using only first two subjects to save time\n",
    "        print(f\"Evaluating {subject}...\")\n",
    "        \n",
    "        subject_results = {}\n",
    "        \n",
    "        # Load test data\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None)\n",
    "        test_df = test_df[:10]  # Use only 10 examples\n",
    "        \n",
    "        for shot_count in shot_counts:\n",
    "            print(f\"Testing with {shot_count} shots...\")\n",
    "            \n",
    "            # Update ntrain parameter\n",
    "            args.ntrain = shot_count\n",
    "            \n",
    "            # Load development data with appropriate number of examples\n",
    "            dev_df = pd.read_csv(os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None)[:args.ntrain]\n",
    "            \n",
    "            # Evaluate model\n",
    "            cors, acc = eval(args, format_subject(subject), dev_df, test_df, model, tokenizer, n_reduced=args.n_reduced)\n",
    "            \n",
    "            subject_results[shot_count] = acc\n",
    "            print(f\"{shot_count}-shot accuracy: {acc * 100:.2f}%\")\n",
    "        \n",
    "        engine_results[subject] = subject_results\n",
    "    \n",
    "    # Store results\n",
    "    shot_results[engine] = engine_results\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Plot shot analysis results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for engine in shot_results:\n",
    "    for subject in shot_results[engine]:\n",
    "        plt.plot(shot_counts, \n",
    "                [shot_results[engine][subject][shot] * 100 for shot in shot_counts], \n",
    "                label=f\"{engine} - {subject.replace('_', ' ').title()}\", \n",
    "                marker='o')\n",
    "\n",
    "plt.xlabel('Number of Few-Shot Examples')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Effect of Shot Count on MMLU Performance')\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.xticks(shot_counts)\n",
    "plt.ylim(0, 50)  # Assuming accuracies are within this range\n",
    "\n",
    "# ## Experiment 4: Reduced Choices Attack\n",
    "# \n",
    "# Testing how models perform when the number of choices is reduced (e.g., from 4 to 2 or 3)\n",
    "\n",
    "# Choose one model for reduced choices analysis\n",
    "args.engine = [\"qwen\"]  # Using Qwen for reduced choices analysis\n",
    "args.reduce_attack = True  # Enable reduce attack\n",
    "\n",
    "reduced_results = {}\n",
    "engines = args.engine\n",
    "\n",
    "for engine in engines:\n",
    "    print(f\"\\n=====================================\")\n",
    "    print(f\"Reduced Choices Analysis for Engine: {engine}\")\n",
    "    print(f\"=====================================\")\n",
    "    \n",
    "    engine_results = {}\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model(args, engine)\n",
    "    \n",
    "    # Move to GPU if not using 8-bit quantization\n",
    "    if not args.load_in_8bit and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    for subject in subjects[:1]:  # Using only one subject to save time\n",
    "        print(f\"Evaluating {subject}...\")\n",
    "        \n",
    "        subject_results = {}\n",
    "        \n",
    "        # Load development and test data\n",
    "        dev_df = pd.read_csv(os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None)[:args.ntrain]\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None)\n",
    "        \n",
    "        # Use only first 5 examples for faster evaluation\n",
    "        test_df = test_df[:5]\n",
    "        \n",
    "        # Test with different numbers of reduced choices\n",
    "        for n_reduced in [2, 3, 4]:  # Original is 4 choices, test with 2, 3, 4 choices\n",
    "            print(f\"Testing with {n_reduced} choices...\")\n",
    "            \n",
    "            args.n_reduced = n_reduced\n",
    "            \n",
    "            # Run evaluation\n",
    "            cors, acc = full_search_eval(args, format_subject(subject), dev_df, test_df, model, tokenizer, n_reduced=n_reduced)\n",
    "            \n",
    "            subject_results[n_reduced] = acc\n",
    "            print(f\"Accuracy with {n_reduced} choices: {acc * 100:.2f}%\")\n",
    "        \n",
    "        engine_results[subject] = subject_results\n",
    "    \n",
    "    # Store results\n",
    "    reduced_results[engine] = engine_results\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Reset reduce_attack flag\n",
    "args.reduce_attack = False\n",
    "\n",
    "# Plot reduced choices results\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for engine in reduced_results:\n",
    "    for subject in reduced_results[engine]:\n",
    "        plt.plot([2, 3, 4], \n",
    "                [reduced_results[engine][subject][n] * 100 for n in [2, 3, 4]], \n",
    "                label=f\"{engine} - {subject.replace('_', ' ').title()}\", \n",
    "                marker='o')\n",
    "\n",
    "plt.xlabel('Number of Choices')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Effect of Number of Choices on Performance')\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.xticks([2, 3, 4])\n",
    "plt.ylim(0, 100)  # Assuming accuracies are within this range\n",
    "\n",
    "# ## Summary of Findings\n",
    "\n",
    "# Print summary table of basic results\n",
    "print(\"\\nModel Performance Summary (5-shot):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Model':<10} | {'Overall Acc (%)':<15} | {'Best Subject':<20} | {'Worst Subject':<20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for engine in basic_results:\n",
    "    subject_accs = basic_results[engine]['subject_accs']\n",
    "    best_subject = max(subject_accs, key=subject_accs.get)\n",
    "    worst_subject = min(subject_accs, key=subject_accs.get)\n",
    "    \n",
    "    print(f\"{engine:<10} | {basic_results[engine]['overall_acc']*100:>13.2f}% | \"\n",
    "          f\"{best_subject.replace('_', ' ').title():<20} | \"\n",
    "          f\"{worst_subject.replace('_', ' ').title():<20}\")\n",
    "\n",
    "# Print position bias summary\n",
    "if position_results:\n",
    "    print(\"\\nPosition Bias Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for engine in avg_position_accs:\n",
    "        print(f\"Model: {engine}\")\n",
    "        print(f\"{'Position':<10} | {'Accuracy (%)':<15}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Sort positions by accuracy (descending)\n",
    "        sorted_positions = sorted(positions, key=lambda pos: avg_position_accs[engine][pos], reverse=True)\n",
    "        \n",
    "        for pos in sorted_positions:\n",
    "            print(f\"{pos:<10} | {avg_position_accs[engine][pos]*100:>13.2f}%\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction and Setup\n",
    "\n",
    "The analysis begins with the setup of necessary libraries and modules for data manipulation, machine learning, and visualization. Custom functions from a module named `LLMs_attack` are imported to facilitate model loading and evaluation.\n",
    "\n",
    "- **Libraries Imported:**\n",
    "  - `os`, `numpy`, `pandas`, `torch`, `matplotlib`, `seaborn`, and others for data handling and visualization.\n",
    "  - Custom functions from `LLMs_attack` for model operations.\n",
    "\n",
    "- **Data Directory Creation:**\n",
    "  - Directories are created to store the MMLU dataset subsets, which are then downloaded for evaluation.\n",
    "\n",
    "- **Custom Args Class:**\n",
    "  - A custom `Args` class is defined to manage configuration parameters for the experiments, such as the number of training examples, models to evaluate, and flags for different attack strategies.\n",
    "\n",
    "### 2. Experiment 1: Basic MMLU Performance Comparison\n",
    "\n",
    "This experiment compares the performance of three models (`gemma`, `llama`, `qwen`) on a subset of the MMLU benchmark. The subjects evaluated include `abstract_algebra`, `anatomy`, `computer_security`, and `high_school_mathematics`.\n",
    "\n",
    "- **Subjects Evaluated:**\n",
    "  - The models are tested on four subjects to assess their performance across different domains.\n",
    "\n",
    "- **Model Evaluation:**\n",
    "  - For each model, the code loads the model and tokenizer, evaluates it on the specified subjects, and stores the results.\n",
    "  - The evaluation uses a 5-shot learning approach, where the model is given five examples to learn from before testing.\n",
    "\n",
    "- **Visualization:**\n",
    "  - The results are visualized using bar charts to compare the performance of each model across different subjects.\n",
    "\n",
    "### 3. Experiment 2: Position Bias Analysis\n",
    "\n",
    "This experiment investigates whether the models exhibit a bias towards choosing answers at specific positions (A, B, C, D). The analysis is performed using the `gemma` model.\n",
    "\n",
    "- **Position Bias:**\n",
    "  - The code evaluates the model's performance when the correct answer is placed at different positions to identify any position bias.\n",
    "\n",
    "- **Visualization:**\n",
    "  - The results are visualized to show the accuracy for each answer position, highlighting any potential biases.\n",
    "\n",
    "### 4. Experiment 3: Shot Analysis\n",
    "\n",
    "This experiment examines how the performance of the `llama` model changes with different numbers of few-shot examples (0, 1, 3, 5).\n",
    "\n",
    "- **Few-Shot Learning:**\n",
    "  - The model's performance is evaluated with varying numbers of few-shot examples to understand the impact of example quantity on accuracy.\n",
    "\n",
    "- **Visualization:**\n",
    "  - The results are visualized to show the impact of the number of few-shot examples on performance.\n",
    "\n",
    "### 5. Experiment 4: Reduced Choices Attack\n",
    "\n",
    "This experiment evaluates the performance of the `qwen` model when the number of choices is reduced from 4 to 2 or 3.\n",
    "\n",
    "- **Reduced Choices:**\n",
    "  - The model's performance is evaluated with a reduced number of choices to understand how it affects accuracy.\n",
    "\n",
    "- **Visualization:**\n",
    "  - The results are visualized to show the impact of reducing the number of choices on performance.\n",
    "\n",
    "### 6. Summary of Findings\n",
    "\n",
    "The analysis concludes with a summary of the findings, including the overall performance of the models and any identified biases.\n",
    "\n",
    "- **Performance Summary:**\n",
    "  - A table summarizes the overall accuracy of each model and highlights the best and worst-performing subjects.\n",
    "\n",
    "- **Position Bias Summary:**\n",
    "  - The position bias analysis is summarized to identify any consistent biases across the models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
