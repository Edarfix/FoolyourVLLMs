{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\todof\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\todof\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models on subjects: ['abstract_algebra', 'anatomy', 'computer_security', 'high_school_mathematics']\n",
      "\n",
      "=====================================\n",
      "Engine: gemma\n",
      "=====================================\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-3-1b-pt.\n401 Client Error. (Request ID: Root=1-67daf66b-2fb07ee41aa983901eb82daa;a312f6b6-51aa-4ff7-9942-627fe09975d2)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-3-1b-pt/resolve/main/config.json.\nAccess to model google/gemma-3-1b-pt is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m         \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/google/gemma-3-1b-pt/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[0;32m    863\u001b[0m             \u001b[1;31m# Destination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;31m# Otherwise, raise appropriate error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[1;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1487\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[0;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[1;31m# Retrieve metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m     r = _request_wrapper(\n\u001b[0m\u001b[0;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"HEAD\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         response = _request_wrapper(\n\u001b[0m\u001b[0;32m    281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\huggingface_hub\\utils\\_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    425\u001b[0m             )\n\u001b[1;32m--> 426\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-67daf66b-2fb07ee41aa983901eb82daa;a312f6b6-51aa-4ff7-9942-627fe09975d2)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-3-1b-pt/resolve/main/config.json.\nAccess to model google/gemma-3-1b-pt is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17060/706103066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# Load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# Move to GPU if not using 8-bit quantization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\OneDrive\\Documents\\MVA\\LLM\\FoolyourVLLMs\\LLMs_attack.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(args, engine)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'gemma'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"google/gemma-3-1b-pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'qwen'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    899\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[0;32m    902\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                     )\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1073\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"code_revision\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1075\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1076\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"auto_map\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"AutoConfig\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"auto_map\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;31m# Get config dict associated with the base config file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m                 \u001b[1;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[0;32m    654\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\todof\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m         raise EnvironmentError(\n\u001b[0m\u001b[0;32m    361\u001b[0m             \u001b[1;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-3-1b-pt.\n401 Client Error. (Request ID: Root=1-67daf66b-2fb07ee41aa983901eb82daa;a312f6b6-51aa-4ff7-9942-627fe09975d2)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-3-1b-pt/resolve/main/config.json.\nAccess to model google/gemma-3-1b-pt is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "# Import necessary functions from the original code\n",
    "from LLMs_attack import (\n",
    "    load_model, eval, gen_prompt, format_example,\n",
    "    move_answers_to_position, format_subject,\n",
    "    full_search_eval, reduce_choices_and_answer\n",
    ")\n",
    "\n",
    "# Create a directory for data\n",
    "os.makedirs('data/MMLU/dev', exist_ok=True)\n",
    "os.makedirs('data/MMLU/test', exist_ok=True)\n",
    "\n",
    "# Download a subset of MMLU dataset\n",
    "!wget -q -O data/MMLU/dev/abstract_algebra_dev.csv https://raw.githubusercontent.com/hendrycks/test/master/data/dev/abstract_algebra_dev.csv\n",
    "!wget -q -O data/MMLU/test/abstract_algebra_test.csv https://raw.githubusercontent.com/hendrycks/test/master/data/test/abstract_algebra_test.csv\n",
    "!wget -q -O data/MMLU/dev/anatomy_dev.csv https://raw.githubusercontent.com/hendrycks/test/master/data/dev/anatomy_dev.csv\n",
    "!wget -q -O data/MMLU/test/anatomy_test.csv https://raw.githubusercontent.com/hendrycks/test/master/data/test/anatomy_test.csv\n",
    "!wget -q -O data/MMLU/dev/computer_security_dev.csv https://raw.githubusercontent.com/hendrycks/test/master/data/dev/computer_security_dev.csv\n",
    "!wget -q -O data/MMLU/test/computer_security_test.csv https://raw.githubusercontent.com/hendrycks/test/master/data/test/computer_security_test.csv\n",
    "!wget -q -O data/MMLU/dev/high_school_mathematics_dev.csv https://raw.githubusercontent.com/hendrycks/test/master/data/dev/high_school_mathematics_dev.csv\n",
    "!wget -q -O data/MMLU/test/high_school_mathematics_test.csv https://raw.githubusercontent.com/hendrycks/test/master/data/test/high_school_mathematics_test.csv\n",
    "\n",
    "# Define custom Args object to pass to functions\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.ntrain = 5  # Number of examples to use for few-shot\n",
    "        self.data_dir = \"data/MMLU\"\n",
    "        self.engine = [\"gemma\", \"llama\", \"qwen\"]  # Models to evaluate\n",
    "        self.n_reduced = None  # No reduction in choices by default\n",
    "        self.use_subset = True  # Use a subset of the test data for faster evaluation\n",
    "        self.permutation_attack = False  # Don't use permutation attack by default\n",
    "        self.position_permute = False  # Don't use position permutation by default\n",
    "        self.reduce_attack = False  # Don't use reduce attack by default\n",
    "        self.load_in_8bit = False  # Don't load in 8-bit by default\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# ## Experiment 1: Basic MMLU Performance Comparison\n",
    "# \n",
    "# We'll compare the performance of three small models on a subset of MMLU benchmark.\n",
    "\n",
    "# Set subjects to evaluate\n",
    "subjects = ['abstract_algebra', 'anatomy', 'computer_security', 'high_school_mathematics']\n",
    "print(f\"Evaluating models on subjects: {subjects}\")\n",
    "\n",
    "# Modify args to use subset\n",
    "args.use_subset = True  # Use only 100 examples from test set\n",
    "args.ntrain = 5  # Use 5-shot examples for few-shot learning\n",
    "\n",
    "# Define results dictionary\n",
    "basic_results = {}\n",
    "\n",
    "# Set the models to evaluate\n",
    "args.engine = [\"gemma\", \"llama\", \"qwen\"]  # Using Qwen as third model instead of Mistral\n",
    "\n",
    "for engine in args.engine:\n",
    "    print(f\"\\n=====================================\")\n",
    "    print(f\"Engine: {engine}\")\n",
    "    print(f\"=====================================\")\n",
    "    \n",
    "    all_cors = []\n",
    "    all_accs = []\n",
    "    subject_results = {}\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model(args, engine)\n",
    "    \n",
    "    # Move to GPU if not using 8-bit quantization\n",
    "    if not args.load_in_8bit and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    for subject in subjects:\n",
    "        print(f\"Evaluating {subject}...\")\n",
    "        \n",
    "        # Load development and test data\n",
    "        dev_df = pd.read_csv(os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None)[:args.ntrain]\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None)\n",
    "        \n",
    "        # Use only first 20 examples for faster evaluation\n",
    "        test_df = test_df[:20]\n",
    "        \n",
    "        # Evaluate model\n",
    "        cors, acc = eval(args, format_subject(subject), dev_df, test_df, model, tokenizer, n_reduced=args.n_reduced)\n",
    "        \n",
    "        # Store results\n",
    "        all_cors.append(cors)\n",
    "        all_accs.append(acc)\n",
    "        subject_results[subject] = acc\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    weighted_acc = np.mean(np.concatenate(all_cors))\n",
    "    print(f\"Average accuracy: {weighted_acc*100:.2f}%\")\n",
    "    \n",
    "    # Store results\n",
    "    basic_results[engine] = {\n",
    "        'subject_accs': subject_results,\n",
    "        'overall_acc': weighted_acc\n",
    "    }\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Plot basic results\n",
    "plt.figure(figsize=(12, 6))\n",
    "engines = list(basic_results.keys())\n",
    "subjects_list = list(basic_results[engines[0]]['subject_accs'].keys())\n",
    "\n",
    "# Create bar chart\n",
    "x = np.arange(len(subjects_list))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "for engine in engines:\n",
    "    offset = width * multiplier\n",
    "    accs = [basic_results[engine]['subject_accs'][subject] * 100 for subject in subjects_list]\n",
    "    plt.bar(x + offset, accs, width, label=engine)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Subjects')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('MMLU Performance by Model and Subject (5-shot)')\n",
    "plt.xticks(x + width, [subject.replace('_', ' ').title() for subject in subjects_list], rotation=45)\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show overall accuracies\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(engines, [basic_results[engine]['overall_acc'] * 100 for engine in engines])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Overall MMLU Accuracy by Model (5-shot)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0, 50)  # Assuming accuracies are within this range\n",
    "\n",
    "# ## Experiment 2: Position Bias Analysis\n",
    "# \n",
    "# Testing if the models have a bias towards choosing answers at specific positions (A, B, C, D)\n",
    "\n",
    "# Choose one model for position bias analysis to save time\n",
    "args.engine = [\"gemma\"]  # Using Gemma for position bias analysis\n",
    "args.position_permute = True  # Enable position permutation\n",
    "\n",
    "position_results = {}\n",
    "engines = args.engine\n",
    "\n",
    "for engine in engines:\n",
    "    print(f\"\\n=====================================\")\n",
    "    print(f\"Position Bias Analysis for Engine: {engine}\")\n",
    "    print(f\"=====================================\")\n",
    "    \n",
    "    all_accs = []\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model(args, engine)\n",
    "    \n",
    "    # Move to GPU if not using 8-bit quantization\n",
    "    if not args.load_in_8bit and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    for subject in subjects:\n",
    "        print(f\"Evaluating {subject}...\")\n",
    "        \n",
    "        # Load development and test data\n",
    "        dev_df = pd.read_csv(os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None)[:args.ntrain]\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None)\n",
    "        \n",
    "        # Use only first 10 examples for faster evaluation\n",
    "        test_df = test_df[:10]\n",
    "        \n",
    "        # Test with answers at different positions\n",
    "        position_accs = {}\n",
    "        for position in ['A', 'B', 'C', 'D']:\n",
    "            print(f\"Testing with answers at position {position}...\")\n",
    "            new_df = move_answers_to_position(test_df, position)\n",
    "            cors, acc = eval(args, format_subject(subject), dev_df, new_df, model, tokenizer, n_reduced=args.n_reduced, permute_pos=position)\n",
    "            position_accs[position] = acc\n",
    "        \n",
    "        all_accs.append(position_accs)\n",
    "        print(f\"Position accuracies for {subject}: {position_accs}\")\n",
    "    \n",
    "    # Store results\n",
    "    position_results[engine] = all_accs\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Reset position_permute flag\n",
    "args.position_permute = False\n",
    "\n",
    "# Plot position bias results\n",
    "plt.figure(figsize=(10, 6))\n",
    "positions = ['A', 'B', 'C', 'D']\n",
    "\n",
    "# Calculate average accuracy for each position across all subjects\n",
    "avg_position_accs = {}\n",
    "for engine in position_results:\n",
    "    avg_position_accs[engine] = {pos: 0 for pos in positions}\n",
    "    for subject_result in position_results[engine]:\n",
    "        for pos in positions:\n",
    "            avg_position_accs[engine][pos] += subject_result[pos]\n",
    "    # Divide by number of subjects\n",
    "    for pos in positions:\n",
    "        avg_position_accs[engine][pos] /= len(subjects)\n",
    "\n",
    "# Create bar chart for position bias\n",
    "for engine in avg_position_accs:\n",
    "    plt.bar(positions, [avg_position_accs[engine][pos] * 100 for pos in positions], label=engine)\n",
    "    \n",
    "plt.xlabel('Answer Position')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Position Bias Analysis: Accuracy by Answer Position')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0, 50)  # Assuming accuracies are within this range\n",
    "\n",
    "# ## Experiment 3: Shot Analysis\n",
    "# \n",
    "# Testing how performance changes with different numbers of few-shot examples\n",
    "\n",
    "# Choose one model for shot analysis\n",
    "args.engine = [\"llama\"]  # Using Llama for shot analysis\n",
    "shot_counts = [0, 1, 3, 5]  # Different numbers of shots to test\n",
    "\n",
    "shot_results = {}\n",
    "engines = args.engine\n",
    "\n",
    "for engine in engines:\n",
    "    print(f\"\\n=====================================\")\n",
    "    print(f\"Shot Analysis for Engine: {engine}\")\n",
    "    print(f\"=====================================\")\n",
    "    \n",
    "    engine_results = {}\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model(args, engine)\n",
    "    \n",
    "    # Move to GPU if not using 8-bit quantization\n",
    "    if not args.load_in_8bit and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    for subject in subjects[:2]:  # Using only first two subjects to save time\n",
    "        print(f\"Evaluating {subject}...\")\n",
    "        \n",
    "        subject_results = {}\n",
    "        \n",
    "        # Load test data\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None)\n",
    "        test_df = test_df[:10]  # Use only 10 examples\n",
    "        \n",
    "        for shot_count in shot_counts:\n",
    "            print(f\"Testing with {shot_count} shots...\")\n",
    "            \n",
    "            # Update ntrain parameter\n",
    "            args.ntrain = shot_count\n",
    "            \n",
    "            # Load development data with appropriate number of examples\n",
    "            dev_df = pd.read_csv(os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None)[:args.ntrain]\n",
    "            \n",
    "            # Evaluate model\n",
    "            cors, acc = eval(args, format_subject(subject), dev_df, test_df, model, tokenizer, n_reduced=args.n_reduced)\n",
    "            \n",
    "            subject_results[shot_count] = acc\n",
    "            print(f\"{shot_count}-shot accuracy: {acc * 100:.2f}%\")\n",
    "        \n",
    "        engine_results[subject] = subject_results\n",
    "    \n",
    "    # Store results\n",
    "    shot_results[engine] = engine_results\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Plot shot analysis results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for engine in shot_results:\n",
    "    for subject in shot_results[engine]:\n",
    "        plt.plot(shot_counts, \n",
    "                [shot_results[engine][subject][shot] * 100 for shot in shot_counts], \n",
    "                label=f\"{engine} - {subject.replace('_', ' ').title()}\", \n",
    "                marker='o')\n",
    "\n",
    "plt.xlabel('Number of Few-Shot Examples')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Effect of Shot Count on MMLU Performance')\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.xticks(shot_counts)\n",
    "plt.ylim(0, 50)  # Assuming accuracies are within this range\n",
    "\n",
    "# ## Experiment 4: Reduced Choices Attack\n",
    "# \n",
    "# Testing how models perform when the number of choices is reduced (e.g., from 4 to 2 or 3)\n",
    "\n",
    "# Choose one model for reduced choices analysis\n",
    "args.engine = [\"qwen\"]  # Using Qwen for reduced choices analysis\n",
    "args.reduce_attack = True  # Enable reduce attack\n",
    "\n",
    "reduced_results = {}\n",
    "engines = args.engine\n",
    "\n",
    "for engine in engines:\n",
    "    print(f\"\\n=====================================\")\n",
    "    print(f\"Reduced Choices Analysis for Engine: {engine}\")\n",
    "    print(f\"=====================================\")\n",
    "    \n",
    "    engine_results = {}\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_model(args, engine)\n",
    "    \n",
    "    # Move to GPU if not using 8-bit quantization\n",
    "    if not args.load_in_8bit and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    for subject in subjects[:1]:  # Using only one subject to save time\n",
    "        print(f\"Evaluating {subject}...\")\n",
    "        \n",
    "        subject_results = {}\n",
    "        \n",
    "        # Load development and test data\n",
    "        dev_df = pd.read_csv(os.path.join(args.data_dir, \"dev\", subject + \"_dev.csv\"), header=None)[:args.ntrain]\n",
    "        test_df = pd.read_csv(os.path.join(args.data_dir, \"test\", subject + \"_test.csv\"), header=None)\n",
    "        \n",
    "        # Use only first 5 examples for faster evaluation\n",
    "        test_df = test_df[:5]\n",
    "        \n",
    "        # Test with different numbers of reduced choices\n",
    "        for n_reduced in [2, 3, 4]:  # Original is 4 choices, test with 2, 3, 4 choices\n",
    "            print(f\"Testing with {n_reduced} choices...\")\n",
    "            \n",
    "            args.n_reduced = n_reduced\n",
    "            \n",
    "            # Run evaluation\n",
    "            cors, acc = full_search_eval(args, format_subject(subject), dev_df, test_df, model, tokenizer, n_reduced=n_reduced)\n",
    "            \n",
    "            subject_results[n_reduced] = acc\n",
    "            print(f\"Accuracy with {n_reduced} choices: {acc * 100:.2f}%\")\n",
    "        \n",
    "        engine_results[subject] = subject_results\n",
    "    \n",
    "    # Store results\n",
    "    reduced_results[engine] = engine_results\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Reset reduce_attack flag\n",
    "args.reduce_attack = False\n",
    "\n",
    "# Plot reduced choices results\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for engine in reduced_results:\n",
    "    for subject in reduced_results[engine]:\n",
    "        plt.plot([2, 3, 4], \n",
    "                [reduced_results[engine][subject][n] * 100 for n in [2, 3, 4]], \n",
    "                label=f\"{engine} - {subject.replace('_', ' ').title()}\", \n",
    "                marker='o')\n",
    "\n",
    "plt.xlabel('Number of Choices')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Effect of Number of Choices on Performance')\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.xticks([2, 3, 4])\n",
    "plt.ylim(0, 100)  # Assuming accuracies are within this range\n",
    "\n",
    "# ## Summary of Findings\n",
    "\n",
    "# Print summary table of basic results\n",
    "print(\"\\nModel Performance Summary (5-shot):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Model':<10} | {'Overall Acc (%)':<15} | {'Best Subject':<20} | {'Worst Subject':<20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for engine in basic_results:\n",
    "    subject_accs = basic_results[engine]['subject_accs']\n",
    "    best_subject = max(subject_accs, key=subject_accs.get)\n",
    "    worst_subject = min(subject_accs, key=subject_accs.get)\n",
    "    \n",
    "    print(f\"{engine:<10} | {basic_results[engine]['overall_acc']*100:>13.2f}% | \"\n",
    "          f\"{best_subject.replace('_', ' ').title():<20} | \"\n",
    "          f\"{worst_subject.replace('_', ' ').title():<20}\")\n",
    "\n",
    "# Print position bias summary\n",
    "if position_results:\n",
    "    print(\"\\nPosition Bias Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for engine in avg_position_accs:\n",
    "        print(f\"Model: {engine}\")\n",
    "        print(f\"{'Position':<10} | {'Accuracy (%)':<15}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Sort positions by accuracy (descending)\n",
    "        sorted_positions = sorted(positions, key=lambda pos: avg_position_accs[engine][pos], reverse=True)\n",
    "        \n",
    "        for pos in sorted_positions:\n",
    "            print(f\"{pos:<10} | {avg_position_accs[engine][pos]*100:>13.2f}%\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction and Setup\n",
    "\n",
    "The analysis begins with the setup of necessary libraries and modules for data manipulation, machine learning, and visualization. Custom functions from a module named `LLMs_attack` are imported to facilitate model loading and evaluation.\n",
    "\n",
    "- **Libraries Imported:**\n",
    "  - `os`, `numpy`, `pandas`, `torch`, `matplotlib`, `seaborn`, and others for data handling and visualization.\n",
    "  - Custom functions from `LLMs_attack` for model operations.\n",
    "\n",
    "- **Data Directory Creation:**\n",
    "  - Directories are created to store the MMLU dataset subsets, which are then downloaded for evaluation.\n",
    "\n",
    "- **Custom Args Class:**\n",
    "  - A custom `Args` class is defined to manage configuration parameters for the experiments, such as the number of training examples, models to evaluate, and flags for different attack strategies.\n",
    "\n",
    "### 2. Experiment 1: Basic MMLU Performance Comparison\n",
    "\n",
    "This experiment compares the performance of three models (`gemma`, `llama`, `qwen`) on a subset of the MMLU benchmark. The subjects evaluated include `abstract_algebra`, `anatomy`, `computer_security`, and `high_school_mathematics`.\n",
    "\n",
    "- **Subjects Evaluated:**\n",
    "  - The models are tested on four subjects to assess their performance across different domains.\n",
    "\n",
    "- **Model Evaluation:**\n",
    "  - For each model, the code loads the model and tokenizer, evaluates it on the specified subjects, and stores the results.\n",
    "  - The evaluation uses a 5-shot learning approach, where the model is given five examples to learn from before testing.\n",
    "\n",
    "- **Visualization:**\n",
    "  - The results are visualized using bar charts to compare the performance of each model across different subjects.\n",
    "\n",
    "### 3. Experiment 2: Position Bias Analysis\n",
    "\n",
    "This experiment investigates whether the models exhibit a bias towards choosing answers at specific positions (A, B, C, D). The analysis is performed using the `gemma` model.\n",
    "\n",
    "- **Position Bias:**\n",
    "  - The code evaluates the model's performance when the correct answer is placed at different positions to identify any position bias.\n",
    "\n",
    "- **Visualization:**\n",
    "  - The results are visualized to show the accuracy for each answer position, highlighting any potential biases.\n",
    "\n",
    "### 4. Experiment 3: Shot Analysis\n",
    "\n",
    "This experiment examines how the performance of the `llama` model changes with different numbers of few-shot examples (0, 1, 3, 5).\n",
    "\n",
    "- **Few-Shot Learning:**\n",
    "  - The model's performance is evaluated with varying numbers of few-shot examples to understand the impact of example quantity on accuracy.\n",
    "\n",
    "- **Visualization:**\n",
    "  - The results are visualized to show the impact of the number of few-shot examples on performance.\n",
    "\n",
    "### 5. Experiment 4: Reduced Choices Attack\n",
    "\n",
    "This experiment evaluates the performance of the `qwen` model when the number of choices is reduced from 4 to 2 or 3.\n",
    "\n",
    "- **Reduced Choices:**\n",
    "  - The model's performance is evaluated with a reduced number of choices to understand how it affects accuracy.\n",
    "\n",
    "- **Visualization:**\n",
    "  - The results are visualized to show the impact of reducing the number of choices on performance.\n",
    "\n",
    "### 6. Summary of Findings\n",
    "\n",
    "The analysis concludes with a summary of the findings, including the overall performance of the models and any identified biases.\n",
    "\n",
    "- **Performance Summary:**\n",
    "  - A table summarizes the overall accuracy of each model and highlights the best and worst-performing subjects.\n",
    "\n",
    "- **Position Bias Summary:**\n",
    "  - The position bias analysis is summarized to identify any consistent biases across the models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
